<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>技術解説：PostgreSQL 17 実行計画(EXPLAIN) 深掘りガイド</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>🔍 DBを「透視」する：実行計画(EXPLAIN) 徹底攻略</h1>
        <nav>
            <ul>
                <li><a href="#intro">1. はじめに</a></li>
                <li><a href="#setup">2. 実験環境構築</a></li>
                <li><a href="#explain-basics">3. EXPLAINの読み方</a></li>
                <li><a href="#index-scan">4. インデックスの真価</a></li>
                <li><a href="#stats">5. 統計情報の力</a></li>
                <li><a href="#join-algo">6. 結合アルゴリズム</a></li>
                <li><a href="#drill">7. 最終演習</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="intro">
            <h2>1. はじめに</h2>
            
            <h3>1.1 なぜ実行計画を学ぶのか？</h3>
            <p>
                SQLは「何を取得するか」を記述する宣言型の言語であり、「どうやって取得するか」の手順はデータベース(DB)にお任せします。
                この「どうやって」を決めるのが<strong>オプティマイザ</strong>であり、その決定事項が<strong>実行計画</strong>です。
            </p>
            <p>
                大規模なシステムでは、実行計画ひとつでクエリの速度が<strong>1000倍以上</strong>変わることも珍しくありません。
                本記事では、PostgreSQL 17のコンテナ環境を使い、10万件のデータに対してインデックスを貼ったり、結合クエリを投げたりしながら、
                プロのエンジニアに必要な「推測と検証」のスキルを身につけます。
            </p>

            <h3>1.2 この記事で学べること</h3>
            <ul>
                <li><code>EXPLAIN</code> と <code>EXPLAIN ANALYZE</code> の違いと使い分け</li>
                <li>Seq Scan、Index Scan、Bitmap Index Scan の違いと発生条件</li>
                <li>オプティマイザが実行計画を選ぶ際の判断基準（コスト理論）</li>
                <li>Nested Loop、Hash Join、Merge Join の3つの結合アルゴリズム</li>
                <li>統計情報（ANALYZE）がオプティマイザに与える影響</li>
                <li>実際のパフォーマンスチューニング方法論</li>
            </ul>

            <div class="important-box">
                <h4>⚠️ 前提知識</h4>
                <p>
                    本記事は、第01〜12回講義で学んだ以下の内容を理解していることを前提とします
                </p>
                <ul>
                    <li>基本的なSELECT文（WHERE、JOIN、GROUP BY、ORDER BY）</li>
                    <li>PRIMARY KEY、FOREIGN KEYの概念</li>
                    <li>INDEXの基礎（CREATE INDEX文の構文）</li>
                    <li>PostgreSQLのDockerコンテナの起動方法</li>
                </ul>
            </div>
        </section>

        <section id="setup">
            <h2>2. 10万件の実験環境構築</h2>
            <p>
                まずは実験の土台となるデータを準備しましょう。
                今回は「ゲームプレイヤー」と「ギルド」の2テーブル構成で、10万件のプレイヤーデータを生成します。
            </p>

            <h3>2.0 環境のセットアップ</h3>
            <p>
                この実験環境では、SQLファイルを簡単に実行できるように、Node.jsのスクリプトを使用します。
                以下の手順で必要なファイルと依存関係をセットアップしてください。
            </p>

            <div class="step-box">
                    以下の<strong>必須ファイルをコピー</strong>してください
                </p>
                <ul>
                    <li><code>package.json</code> - NPMパッケージ設定ファイル</li>
                    <li><code>tsconfig.json</code> - TypeScript設定ファイル</li>
                    <li><code>scripts/</code> フォルダ - SQLランナースクリプト</li>
                    <li><code>sql/</code> フォルダ - 実験用SQLファイル</li>
                </ul>
                <p class="note">
                    💡 <code>npm install</code> で 
                    <code>ENOENT: no such file or directory, open '...実行計画1\package.json'</code> 
                    というエラーが出た場合、上記のファイルをコピーしてから再度実行してください。
                </p>
            </div>

            <div class="step-box">
                <h4>📦 初回セットアップ手順</h4>
                <ol>
                    <li><strong>PostgreSQL 17のDockerコンテナを起動</strong>
                        <pre>cd ../DB-PostgreSQL
npm run db:up</pre>
                    </li>
                    <li><strong>実行計画フォルダに移動</strong>
                        <pre>cd ../実行計画</pre>
                    </li>
                    <li><strong>依存関係をインストール</strong>（初回のみ必要）
                        <pre>npm install</pre>
                        <p class="note">これにより、TypeScriptランタイム（tsx）などがインストールされます。</p>
                    </li>
                    <li><strong>動作確認：セットアップSQLを実行</strong>
                        <pre>npm run sql sql/exp_setup.sql</pre>

                    </li>
                </ol>
            </div>

            <div class="important-box">
                <h4>💡 使用する主要コマンド</h4>
                <ul>
                    <li><code>npm run sql &lt;ファイルパス&gt;</code> - 指定したSQLファイルを実行</li>
                    <li><code>npm run sql:en &lt;ファイルパス&gt;</code> - エラーメッセージを英語で表示</li>
                    <li><code>npm run db:up</code> - PostgreSQLコンテナを起動</li>
                    <li><code>npm run db:down</code> - PostgreSQLコンテナを停止</li>
                </ul>
                <p class="warning">
                    ⚠️ <strong>重要：</strong> <code>npm run sql</code> コマンドは、PostgreSQL 17のDockerコンテナ（コンテナ名: <code>pg17</code>）に接続して
                    SQLを実行します。実行前にコンテナが起動していることを必ず確認してください。
                </p>
            </div>

            <h3>2.1 テーブル定義とデータ生成</h3>
            <div class="code-block">
                <p class="filename">📁 sql/exp_setup.sql</p>
                <pre>-- クリーンアップ
DROP TABLE IF EXISTS players CASCADE;
DROP TABLE IF EXISTS guilds CASCADE;

-- ギルドテーブル（親テーブル）
CREATE TABLE guilds (
    id INT PRIMARY KEY,
    guild_name TEXT NOT NULL,
    region TEXT NOT NULL,
    founded_at TIMESTAMP NOT NULL
);

-- プレイヤーテーブル（子テーブル）
CREATE TABLE players (
    id INT PRIMARY KEY,
    name TEXT NOT NULL,
    level INT NOT NULL,
    guild_id INT REFERENCES guilds(id),
    login_count INT DEFAULT 0,
    last_login TIMESTAMP,
    created_at TIMESTAMP NOT NULL
);

-- ギルドデータ(50件)
INSERT INTO guilds 
SELECT 
    i, 
    'Guild_' || i, 
    (ARRAY['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka', 'Sapporo'])[floor(random()*5)+1],
    '2023-01-01'::timestamp + (random() * interval '365 days')
FROM generate_series(1, 50) i;

-- プレイヤーデータ(10万件)
-- 現実的なレベル分布：初心者30%、中級者40%、上級者30%
INSERT INTO players
SELECT 
    i, 
    'Player_' || i, 
    CASE 
        WHEN random() < 0.3 THEN floor(random() * 30 + 1)
        WHEN random() < 0.7 THEN floor(random() * 40 + 31)
        ELSE floor(random() * 30 + 71)
    END,
    CASE WHEN random() < 0.9 THEN floor(random() * 50 + 1) ELSE NULL END,
    floor(random() * 500),
    CASE WHEN random() < 0.8 THEN '2024-01-01'::timestamp + (random() * interval '365 days') ELSE NULL END,
    '2023-01-01'::timestamp + (random() * interval '730 days')
FROM generate_series(1, 100000) i;

-- 統計情報を最新化（必須）
ANALYZE guilds;
ANALYZE players;

-- データ確認
SELECT COUNT(*) as guild_count FROM guilds;
SELECT COUNT(*) as player_count FROM players;</pre>
            </div>

            <div class="step-box">
                <h4>🚀 実行手順</h4>
                <ol>
                    <li>PostgreSQL 17のDockerコンテナが起動していることを確認</li>
                    <li>上記のSQLを <code>sql/exp_setup.sql</code> として保存</li>
                    <li>ターミナルで実行：<code>npm run sql sql/exp_setup.sql</code></li>
                </ol>
            </div>

            <h3>2.2 データ分布の確認</h3>
            <div class="code-block">
                <pre>-- レベル分布の確認
SELECT 
    CASE 
        WHEN level BETWEEN 1 AND 30 THEN '初心者(1-30)'
        WHEN level BETWEEN 31 AND 70 THEN '中級者(31-70)'
        ELSE '上級者(71-100)'
    END as category,
    COUNT(*) as count,
    ROUND(COUNT(*)*100.0/(SELECT COUNT(*) FROM players), 1) || '%' as ratio
FROM players
GROUP BY category
ORDER BY MIN(level);</pre>
            </div>
        </section>

        <section id="explain-basics">
            <h2>3. EXPLAINの読み方とコスト理論</h2>
            
            <h3>3.1 EXPLAIN と EXPLAIN ANALYZE の違い</h3>
            <p>PostgreSQLには2つの実行計画確認方法があります。</p>
            
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>コマンド</th>
                        <th>役割</th>
                        <th>実行時間</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>EXPLAIN</strong></td>
                        <td>オプティマイザの「予測」を表示</td>
                        <td>高速（クエリを実行しない）</td>
                    </tr>
                    <tr>
                        <td><strong>EXPLAIN ANALYZE</strong></td>
                        <td>実際に実行して「実測値」を表示</td>
                        <td>低速（実際にクエリを実行）</td>
                    </tr>
                </tbody>
            </table>

            <h3>3.2 Seq Scan（全件走査）の実行計画を見る</h3>
            <div class="code-block">
                <pre>-- インデックスがない状態での実行計画（予測）
EXPLAIN SELECT * FROM players WHERE level = 50;

-- 実際に実行して実測値を確認
EXPLAIN ANALYZE SELECT * FROM players WHERE level = 50;</pre>
            </div>
            <div class="code-explanation">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>1行目（EXPLAINコマンド）：</strong> EXPLAINはクエリを実行せず、オプティマイザが「これぐらいのコストがかかると予想している」という理論値を返します。実際のデータベース操作は行いません。</li>
                    <li><strong>4行目（EXPLAIN ANALYZEコマンド）：</strong> ANALYZEをつけるとクエリを実際に実行し、実際にかかった時間（actual time）、実際に取得した行数（actual rows）などの実測値が得られます。デバッグの際はこちらが非常に重要です。</li>
                    <li><strong>WHERE level = 50：</strong> この条件では、全100,000行をすべて検索する必要があり、インデックスがなければ全件スキャンが発生します。</li>
                </ul>
            </div>

            <div class="result">
                <h4>📊 出力例（EXPLAIN）</h4>
                <pre>Seq Scan on players  (cost=0.00..2041.00 rows=1000 width=45)
  Filter: (level = 50)</pre>
                
                <h4>📊 出力例（EXPLAIN ANALYZE）</h4>
                <pre>Seq Scan on players  (cost=0.00..2041.00 rows=1000 width=45) (actual time=0.023..14.567 rows=990 loops=1)
  Filter: (level = 50)
  Rows Removed by Filter: 99010
Planning Time: 0.112 ms
Execution Time: 14.612 ms</pre>
            </div>

            <h3>3.3 実行計画の各要素を理解する</h3>
            <ul>
                <li><strong>Seq Scan：</strong> Sequential Scan（順次走査）。テーブルを先頭から順番に読み込み</li>
                <li><strong>cost=0.00..2041.00：</strong> 開始コスト..終了コスト（単位は相対的な「コスト値」）</li>
                <li><strong>rows=1000：</strong> 予測される返却行数</li>
                <li><strong>width=45：</strong> 1行当たりの平均バイト数</li>
                <li><strong>actual time=0.023..14.567：</strong> 実測の開始時間..終了時間（ミリ秒）</li>
                <li><strong>actual rows=990：</strong> 実際に返された行数</li>
                <li><strong>Rows Removed by Filter：</strong> WHERE句で除外された行数</li>
                <li><strong>Execution Time：</strong> 全体の実行時間</li>
            </ul>

            <h3>3.4 コスト計算の仕組み</h3>
            <p>PostgreSQLのコストは以下のパラメータから算出されます。</p>
            <div class="formula-box">
                <pre>Seq Scanのコスト = (ページ数 × seq_page_cost) + (行数 × cpu_tuple_cost) + (行数 × cpu_operator_cost)</pre>
                <p>デフォルト値</p>
                <ul>
                    <li><code>seq_page_cost = 1.0</code> (順次ディスク読み込みコスト)</li>
                    <li><code>random_page_cost = 4.0</code> (ランダムディスク読み込みコスト)</li>
                    <li><code>cpu_tuple_cost = 0.01</code> (1行処理のCPUコスト)</li>
                    <li><code>cpu_operator_cost = 0.0025</code> (演算1回のコスト)</li>
                </ul>
            </div>

            <div class="experiment-box">
                <h4>🧪 実験1：予測と実測のズレを観察</h4>
                <p>以下のクエリを実行し、予測値（rows）と実測値（actual rows）を比較してください。</p>
                <div class="code-block">
                    <pre>EXPLAIN ANALYZE SELECT * FROM players WHERE level = 100;
EXPLAIN ANALYZE SELECT * FROM players WHERE level >= 1 AND level <= 10;
EXPLAIN ANALYZE SELECT * FROM players WHERE guild_id = 5;</pre>
                </div>                <div class=\"code-explanation\">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>1行目（WHERE level = 100）：</strong> 特定の単一値を指定。オプティマイザは統計情報から「この値は約1,000行あるだろう」と予測しますが、実際に100が出現する度数が予測と異なると、実測値とズレます</li>
                        <li><strong>2行目（WHERE level >= 1 AND level <= 10）：</strong> 範囲条件です。初心者レベルの絞り込みなので、実際には少数行かもしれません。統計情報が古い場合、大きなズレが発生します</li>
                        <li><strong>3行目（WHERE guild_id = 5）：</strong> ギルド5に属するプレイヤーの数。各ギルドの人数分布が均等でない場合、予測値と実測値が大きく異なり、実行計画の選択が間違うことがあります</li>
                    </ul>
                </div>                <p><strong>考察：</strong> 予測値と実測値にズレがある場合、その理由は何でしょうか？</p>
            </div>
        </section>

        <section id="index-scan">
            <h2>4. インデックスの真価と「使われない」ケース</h2>
            
            <h3>4.1 インデックス作成と効果測定</h3>
            <div class="code-block">
                <pre>-- 【Before】 インデックスなし
EXPLAIN ANALYZE SELECT * FROM players WHERE level = 50;

-- インデックスを作成
CREATE INDEX idx_players_level ON players(level);

-- 【After】 インデックスあり
EXPLAIN ANALYZE SELECT * FROM players WHERE level = 50;</pre>
            </div>            <div class=\"code-explanation\">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>2行目（Beforeの実行計画）：</strong> インデックス作成前は、全100,000行をすべてスキャンする「Seq Scan」が選ばれます。10万行を一つずつフィルターするため、時間がかかります</li>
                    <li><strong>5行目（CREATE INDEX）：</strong> level列にインデックスを作成します。このインデックスは「level = X のデータはファイルのどこにあるか」を高速に検索できるB-treeデータ構造です</li>
                    <li><strong>8行目（Afterの実行計画）：</strong> インデックス作成後、PostgreSQLのオプティマイザは「インデックスを使ったほうが安い」と判断し、「Index Scan」に自動的に切り替わります。10倍以上の高速化が期待できます</li>
                </ul>
            </div>
            <div class="result">
                <h4>📊 実行結果（インデックス作成前）</h4>
                <pre>Execution Time: 14.612 ms</pre>
                <h4>📊 実行結果（インデックス作成後）</h4>
                <pre>Execution Time: 0.832 ms</pre>
                <p><strong>高速化：約17.6倍！</strong></p>
            </div>

            <h3>4.2 3つのスキャン方式を理解する</h3>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>スキャン方式</th>
                        <th>対象行数</th>
                        <th>特徴</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Seq Scan</strong></td>
                        <td>全体の5%以上</td>
                        <td>テーブル全体を順次読み込み</td>
                    </tr>
                    <tr>
                        <td><strong>Index Scan</strong></td>
                        <td>1〜数十行</td>
                        <td>インデックスから直接取得（1件ずつ）</td>
                    </tr>
                    <tr>
                        <td><strong>Bitmap Index Scan</strong></td>
                        <td>数百〜数千行</td>
                        <td>インデックスから効率的にページをまとめて取得</td>
                    </tr>
                </tbody>
            </table>

            <div class="experiment-box">
                <h4>🧪 実験2：インデックス使用の境界線</h4>
                <p>以下の3クエリで、どのスキャン方式が選ばれるか観察してください。</p>
                <div class="code-block">
                    <pre>-- A: 絞り込みが強い（対象が少ない）
EXPLAIN ANALYZE SELECT * FROM players WHERE level = 100;

-- B: 絞り込みが中程度
EXPLAIN ANALYZE SELECT * FROM players WHERE level BETWEEN 30 AND 70;

-- C: 絞り込みが弱い（対象が多い）
EXPLAIN ANALYZE SELECT * FROM players WHERE level >= 1;</pre>
                </div>                <div class=\"code-explanation\">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>A: WHERE level = 100：</strong> レベル100は、上級者の中でも最高レベルです。全体の約1%未満と予想でき、インデックスが有効です。「Index Scan」を選びます</li>
                        <li><strong>B: WHERE level BETWEEN 30 AND 70：</strong> 中級者と初心者上位を指定。全体の約40～50%に該当し、インデックスを使うメリットが減少します。この境界では「Bitmap Index Scan」が選ばれる可能性があります</li>
                        <li><strong>C: WHERE level >= 1：</strong> ほぼ全行に該当します。インデックスを経由してランダムアクセスするコストが、全行を順序よく読むコストを上回り、「Seq Scan」に切り替わります。これが「コストベース最適化」の本質です</li>
                    </ul>
                </div>                <p><strong>予想：</strong> CはSeq Scanに変わるはずです。なぜでしょうか？</p>
            </div>

            <div class="important-box">
                <h4>💡 重要な原則：コストベースの判断</h4>
                <p>
                    全体の約5%以上を取得する場合、PostgreSQLはSeq Scanを選びます。
                    これは<code>random_page_cost (4.0) > seq_page_cost (1.0)</code>だからです。
                </p>
                <p>
                    インデックス経由でデータを取得する流れ<br>
                    1. インデックスを探索<br>
                    2. 該当アドレスを取得<br>
                    3. テーブル本体からランダムアクセス（重い）<br>
                </p>
                <p>
                    対象行が多いと、ステップ3の負荷が膨大になり、全部読んだほうが速くなります。
                </p>
            </div>

            <h3>4.3 インデックスが使えないSQLパターン</h3>
            <div class="code-block">
                <pre>-- ❌ 悪い例1：インデックス列に関数を適用
EXPLAIN SELECT * FROM players WHERE LOWER(name) = 'player_100';

-- ❌ 悪い例2：インデックス列に演算
EXPLAIN SELECT * FROM players WHERE level * 2 > 100;

-- ❌ 悪い例3：型変換
EXPLAIN SELECT * FROM players WHERE name = 100;

-- ✅ 良い例1：関数を右辺に
EXPLAIN SELECT * FROM players WHERE name = LOWER('PLAYER_100');

-- ✅ 良い例2：演算を右辺に
EXPLAIN SELECT * FROM players WHERE level > 100 / 2;</pre>
            </div>            <div class=\"code-explanation\">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>2行目（LOWER(name) = ...）：</strong> インデックスはname列の「元々の値」に対して構築されています。LOWER()関数を適用した値を検索しているので、インデックスが使えず、Seq Scanになります。関数インデックスを別途作成する必要があります</li>
                    <li><strong>5行目（level * 2 > 100）：</strong> level列に「*2」という演算を適用しているため、インデックスが使えません。WHERE句でlevel列そのものに条件がないので、インデックスの恩恵が受けられません</li>
                    <li><strong>8行目（name = 100）：</strong> name列はTEXT型ですが、右辺には数値を比較しています。型変換が行われるため、インデックスが使えなくなる可能性があります</li>
                    <li><strong>11行目（正しい例1）：</strong> 関数をLOWER('PLAYER_100')のように「定数に対して」適用し、インデックス列LOWER(name)と比較します。関数インデックスがあれば使用されます</li>
                    <li><strong>14行目（正しい例2）：</strong> 演算を「右辺」で行い、インデックス列「level」はそのままです。level > 50 という形になり、インデックスが有効に働きます</li>
                </ul>
            </div>
            <div class="confirmation-box">
                <h4>📝 定着確認1：実行計画の判断</h4>
                <ol>
                    <li>インデックスがあれば、常にSeq Scanより高速である（⭕/❌）</li>
                    <li>Bitmap Index Scanは、取得行数が少ない場合に使われる（⭕/❌）</li>
                    <li>WHERE句でインデックス列に関数を適用すると、インデックスが使われない（⭕/❌）</li>
                </ol>
                <details>
                    <summary>解答を表示</summary>
                    <p>1. ❌（取得行数が多い場合はSeq Scanのほうが速い）</p>
                    <p>2. ❌（逆です。Bitmap Index Scanは対象行数が多い場合に使われます）</p>
                    <p>3. ⭕（関数インデックスを使えば解決可能）</p>
                </details>
            </div>
        </section>

        <section id="stats">
            <h2>5. 統計情報とオプティマイザの「賢さ」</h2>
            
            <h3>5.1 統計情報とは</h3>
            <p>
                オプティマイザが正確な実行計画を立てるには、テーブルの<strong>統計情報</strong>が必須です。
                統計情報には、テーブルの行数、カラムの値の分布、カラム間の相関などが含まれます。
            </p>

            <h3>5.2 統計情報を確認する</h3>
            <div class="code-block">
                <pre>-- テーブル全体の統計
SELECT 
    schemaname,
    tablename,
    n_live_tup as row_count,
    n_dead_tup as dead_rows,
    last_vacuum,
    last_analyze
FROM pg_stat_user_tables 
WHERE tablename = 'players';

-- カラムレベルの統計
SELECT 
    attname,
    n_distinct,
    avg_width,
    null_frac
FROM pg_stats 
WHERE table_name = 'players' 
ORDER BY attname;</pre>
            </div>
            <div class="code-explanation">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>4～5行目（n_live_tup, n_dead_tup）：</strong> 「今現在テーブルに存在する行数」と「DELETE後に未削除のゴミ行数」です。このrow_countを基に、オプティマイザは「全体何行のうち、何行が該当するか」の比率を計算します</li>
                    <li><strong>6～7行目（last_vacuum, last_analyze）：</strong> 前回のVACUUM/ANALYZEの日時です。この値が古い場合、統計情報が「古い」状態であり、実行計画が間違う原因になります。オプティマイザの判断精度は統計情報の「鮮度」に依存します</li>
                    <li><strong>14～17行目（カラムレベルの統計）：</strong> 「n_distinct」は「そのカラムにいくつ異なる値が存在するか」の推定値です。例えば、level列のn_distinctが100なら、「100種類の異なるレベル値がある」という意味で、これが「level=50」でどれぐらいの行がヒットするかの推測に使われます。「avg_width」は「1行あたりのバイト数」で、テーブルスキャンのコスト計算に使われます。「null_frac」は「NULLの割合」です</li>
                </ul>
            </div>

            <h3>5.3 ANALYZE コマンドの重要性</h3>
            <p>
                大量の<code>INSERT</code>や<code>DELETE</code>を行った直後は、統計情報が古くなります。
                その場合、オプティマイザが「間違った計画」を立ててしまいます。
            </p>

            <div class="experiment-box">
                <h4>🧪 実験3：統計情報の影響を観察</h4>
                <div class="code-block">
                    <pre>-- 1. 100万件の新規プレイヤーを挿入
INSERT INTO players (id, name, level, guild_id, login_count, created_at)
SELECT 
    100000 + i,
    'NewPlayer_' || i,
    floor(random() * 100 + 1),
    floor(random() * 50 + 1),
    0,
    CURRENT_TIMESTAMP
FROM generate_series(1, 100000) i;

-- 2. ANALYZEなしで実行計画を確認
EXPLAIN SELECT * FROM players WHERE level = 50;

-- 3. ANALYZEを実行
ANALYZE players;

-- 4. 実行計画を再確認
EXPLAIN SELECT * FROM players WHERE level = 50;</pre>
                </div>
                <div class="code-explanation">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>2～10行目（INSERT文）：</strong> 「100000 + i」により、ID 100001～200000の新規プレイヤーを100万件追加します。統計情報は更新されていないため、オプティマイザはまだ「元の10万行」だと思い込んでいます</li>
                        <li><strong>13行目（ANALYZEなしの実行計画）：</strong> この段階では、テーブルには20万行があるのに、統計情報は「10万行のまま」です。したがって、「rows = 1000」という古い予測値のままで、間違った実行計画の選択につながります</li>
                        <li><strong>16行目（ANALYZEコマンド）：</strong> 統計情報をスキャンして最新化します。PostgreSQLがテーブルをサンプリングして「今は20万行あり、各レベルの分布はこのようになっている」という情報を更新します</li>
                        <li><strong>19行目（ANALYZE後の実行計画）：</strong> 統計情報が更新された後、WHERE level = 50の「rows」は「2000」のように新しい値に変わります。これで、オプティマイザはより正確な実行計画を選べるようになります</li>
                    </ul>
                </div>
                <p><strong>観察：</strong> <code>rows</code> の予測値がどう変わりましたか？</p>
            </div>

            <div class="important-box">
                <h4>💡 定期的な ANALYZE の重要性</h4>
                <p>
                    大規模テーブルでは、定期的に<code>ANALYZE</code>を実行する必要があります。
                    PostgreSQL 10以降は、自動ANALYZEが有効になっているため、通常は手動実行が不要です。
                </p>
                <p>
                    大量のDML実行直後や、本番環境でのチューニングが必要な場合は、
                    手動で<code>ANALYZE</code>を実行してください。
                </p>
            </div>
        </section>

        <section id="join-algo">
            <h2>6. 結合アルゴリズムの正体</h2>
            
            <h3>6.1 3つの結合方式</h3>
            <p>
                データベースは、2つのテーブルを結合する際に、3種類のアルゴリズムから最適なものを選びます。
            </p>

            <h3>6.2 (1) Nested Loop Join</h3>
            <p>
                外側テーブルの各行について、内側テーブルを全走査する「二重ループ」方式です。
                少量のデータ結合に向いています。
            </p>
            <div class="code-block">
                <pre>-- Nested Loopを誘発（特定1プレイヤーのギルド情報）
EXPLAIN ANALYZE 
SELECT p.name, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id 
WHERE p.id = 1;</pre>
            </div>
            <div class="code-explanation">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>2～6行目（Nested Loopクエリ構造）：</strong> 「WHERE p.id = 1」により、playersテーブルから「1行だけ」取得します。その後、その1行のguild_idを使って、guildsテーブルから対応するギルド情報を取得します。「外側が1行」のため、Nested Loopは非常に効率的です</li>
                    <li><strong>結果の「Index Scan using players_pkey」：</strong> PRIMARY KEY（id）のインデックスを使って、プレイヤーID=1のレコードを「1行」だけ高速に取得します。これが外側ループになります</li>
                    <li><strong>結果の「Index Scan using guilds_pkey on guilds g」：</strong> 取得したプレイヤーのguild_idを使って、guildsテーブルのPRIMARY KEYインデックスでギルド情報を検索します。「1行のプレイヤーに対して1回の検索」なので、Nested Loopでも高速です</li>
                </ul>
            </div>
            <div class="result">
                <pre>Nested Loop  (cost=0.29..15.20 rows=1 width=...)
  -&gt;  Index Scan using players_pkey on players p
  -&gt;  Index Scan using guilds_pkey on guilds g</pre>
            </div>

            <h3>6.3 (2) Hash Join</h3>
            <p>
                小さいテーブルをメモリ上にハッシュテーブルとして構築し、
                大きいテーブルをスキャンしながら検索する方式です。大量データ結合に最適。
            </p>
            <div class="code-block">
                <pre>-- Hash Joinを誘発（全プレイヤーとギルドの結合）
EXPLAIN ANALYZE 
SELECT p.name, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id;</pre>
            </div>
            <div class="code-explanation">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>2～5行目（Hash Joinの条件）：</strong> WHERE句がないため「全プレイヤー（約10万行）と全ギルド（50行）の結合」になります。90,000行の結合（WHERE句なしだと全プレイヤーがギルドに属する）になり、Nested Loopでは「10万×50の検索回数」になって遅いため、Hash Joinが選ばれます</li>
                    <li><strong>「Hash Cond: (p.guild_id = g.id)」：</strong> 結合条件を指定しています。小さいテーブル（guilds: 50行）をハッシュテーブルに変換し、大きいテーブル（players: 10万行）の各行のguild_idでハッシュ検索します</li>
                    <li><strong>「Seq Scan on players p」：</strong> 全10万行を順序よく読み込みます。その各行について、ハッシュテーブル内で「このguild_idのギルド情報」を「O(1)の高速検索」で取得します</li>
                    <li><strong>「Hash (... guilds g)」：</strong> 50行のギルドテーブルをメモリ上のハッシュテーブルに構築します。「メモリがあれば」Hash Joinは大量結合で非常に高速です。ただしメモリが不足すると、ディスクにあふれて遅くなります</li>
                </ul>
            </div>
            <div class="result">
                <pre>Hash Join  (cost=2.00..5432.10 rows=90000 width=...)
  Hash Cond: (p.guild_id = g.id)
  -&gt;  Seq Scan on players p
  -&gt;  Hash  (cost=1.00..1.00 rows=50 width=...)
       -&gt;  Seq Scan on guilds g</pre>
            </div>

            <h3>6.4 (3) Merge Join</h3>
            <p>
                両テーブルが結合キーでソート済み（またはインデックスがある）場合、
                テープドライブのマージソートのように効率的に結合します。
            </p>
            <div class="code-block">
                <pre>-- 複合インデックスを作成
CREATE INDEX idx_players_guild_level ON players(guild_id, level);

-- Merge Joinを誘発（ソート条件が満たされる）
EXPLAIN ANALYZE 
SELECT p.id, p.name, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id 
ORDER BY p.guild_id, p.level;</pre>
            </div>
            <div class="code-explanation">
                <h5>📍 コード解説</h5>
                <ul>
                    <li><strong>2行目（複合インデックス作成）：</strong> 「(guild_id, level)」の順で「ソート済み」のインデックスを作成します。これはplayersテーブルのデータが「guild_idで整理され、その中でlevelでソート」された状態になります。この「ソート済みインデックス」があるかどうかが、Merge Joinが選ばれるかの大きな要因です</li>
                    <li><strong>6～9行目（ORDER BY guild_id, level）：</strong> 「ORDER BY」でguild_idとlevelでソート結果を求めています。オプティマイザは「あ、playersテーブルはすでに(guild_id, level)でソート済みのインデックスがある」と認識し、「新たにソートする必要がない」と判断します。これがMerge Joinを使う条件になります</li>
                    <li><strong>結果的な効率性：</strong> Merge Joinは「ソート済みの左テーブルと右テーブルを、2本のポインタで同時にスキャン」する方式で、ソート不要なため「Hash Joinより高速」になり得ます。特にメモリが余っていない環境、大量結合でソート結果が必要な場合に有効です</li>
                </ul>
            </div>

            <div class="confirmation-box">
                <h4>📝 定着確認2：結合方式の選択</h4>
                <p>以下のシナリオで、どの結合方式が選ばれやすいか答えてください。</p>
                <ol>
                    <li>小テーブル（100行）× 大テーブル（100万行）の結合：？</li>
                    <li>大テーブル（100万行）× 大テーブル（100万行）の結合：？</li>
                    <li>小テーブル（10行）× 小テーブル（50行）の結合：？</li>
                </ol>
                <details>
                    <summary>解答を表示</summary>
                    <p>1. Hash Join（メモリに小テーブルを展開するため高速）</p>
                    <p>2. Hash Join（またはMerge Join if ソート済み）</p>
                    <p>3. Nested Loop（テーブルサイズが小さいため有効）</p>
                </details>
            </div>
        </section>

        <section id="drill">
            <h2>7. SQLドリル：最終演習（解答例付き）</h2>
            
            <h3>【演習1】スロークエリを救え（難易度：⭐⭐）</h3>
            <div class="drill-box">
                <p>
                    以下のクエリは現在 <code>Seq Scan</code> で非常に低速です。
                    実行計画を確認し、適切なインデックスを作成して <code>Index Scan</code> に変更してください。
                    <strong>実行時間を1/5以下にすることが目標です。</strong>
                </p>
                <div class="code-block">
                    <pre>SELECT name, created_at 
FROM players 
WHERE created_at >= '2023-12-01' 
  AND login_count > 300;</pre>
                </div>
            </div>

            <details>
                <summary>演習1の解答例を表示</summary>
                <div class="code-block">
                    <pre>-- 1. 現状を確認
EXPLAIN ANALYZE 
SELECT name, created_at 
FROM players 
WHERE created_at >= '2023-12-01' 
  AND login_count > 300;

-- 2. 複合インデックスを作成（等号や範囲条件の順序に注意）
CREATE INDEX idx_players_created_login ON players(created_at, login_count);

-- 3. 再度実行計画を確認
EXPLAIN ANALYZE 
SELECT name, created_at 
FROM players 
WHERE created_at >= '2023-12-01' 
  AND login_count > 300;</pre>
                </div>
                <div class="code-explanation\">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>2～6行目（現状確認）：</strong> 2つの範囲条件（created_at >= と login_count > ）でフィルターしています。現在はSeq Scanのため、全10万行を読む必要があります</li>
                        <li><strong>9行目（複合インデックス作成）：</strong> (created_at, login_count)の複合インデックスを作成します。created_atはより選択性が高い（日付は多くのバリエーション）ため、左に配置するのが最適です</li>
                        <li><strong>12～16行目（改善後の確認）：</strong> インデックスが作成された後、同じクエリを実行すると、Bitmap Index ScanまたはIndex Scanに切り替わり、実行時間が大幅に短縮されます</li>
                    </ul>
                </div>
                <p>
                    <strong>解説：</strong> 
                    このクエリは両方とも範囲条件なので、created_at が より選択性が高いため（日付は多くのバリエーション）、
                    created_at を左に配置するのが最適です。
                </p>
            </details>

            <h3>【演習2】インデックスが使われない理由を特定（難易度：⭐⭐⭐）</h3>
            <div class="drill-box">
                <p>
                    <code>idx_players_level</code> インデックスが存在するにも関わらず、
                    以下のクエリは <code>Seq Scan</code> になります。
                    なぜ使われないのか理由を述べ、
                    インデックスが使われるようにSQLを書き直してください。
                </p>
                <div class="code-block">
                    <pre>-- idx_players_level インデックスがあるのに使われない
SELECT * FROM players WHERE level * 2 > 180;</pre>
                </div>
            </div>

            <details>
                <summary>演習2の解答例を表示</summary>
                <div class="code-block">
                    <pre>-- 【原因】
-- インデックス列(level)に対して演算(*2)を行っているため、
-- インデックスが使用不可になります。

-- 【修正案1】右辺で計算
EXPLAIN ANALYZE 
SELECT * FROM players WHERE level > 180 / 2;  -- level > 90

-- 【修正案2】型が安全な書き方
EXPLAIN ANALYZE 
SELECT * FROM players WHERE level > 90;</pre>
                </div>
                <div class="code-explanation">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>1～3行目（原因説明）：</strong> level列に「*2」という演算を適用しているため、インデックスが「使用不可」になります。PostgreSQLのオプティマイザは「(level * 2)という式のインデックスは存在しない」と判断し、Seq Scanを選びます</li>
                        <li><strong>6～8行目（修正案1）：</strong> 「level > 180 / 2」に書き換えます。クエリプランナーは「level > 90」（180/2の計算結果）と理解し、idx_players_levelインデックスを使用可能と判断します</li>
                        <li><strong>11～12行目（修正案2）：</strong> 最も単純で確実な書き方です。人間が計算を済ませて「level > 90」と書くことで、インデックスが確実に使われます</li>
                    </ul>
                </div>
                <p>
                    <strong>解説：</strong> 
                    インデックスは「カラムの値そのもの」に対して構築されているため、
                    カラムに関数や演算を適用するとインデックスが使えなくなります。
                    右辺で計算を済ませると、インデックスが有効になります。
                </p>
            </details>

            <h3>【演習3】結合クエリのチューニング（難易度：⭐⭐⭐）</h3>
            <div class="drill-box">
                <p>
                    以下のクエリをEXPLAIN ANALYZEで実行し、
                    どの結合方式が選ばれているか答えてください。
                    また、Hash Joinではなく Merge Join を使うようにチューニングしてください。
                </p>
                <div class="code-block">
                    <pre>SELECT p.id, p.name, p.level, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id 
WHERE p.level >= 50 
ORDER BY p.guild_id, p.level;</pre>
                </div>
            </div>

            <details>
                <summary>演習3の解答例を表示</summary>
                <div class="code-block">
                    <pre>-- 1. 現状確認（Hash Joinが選ばれる可能性が高い）
EXPLAIN ANALYZE 
SELECT p.id, p.name, p.level, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id 
WHERE p.level >= 50 
ORDER BY p.guild_id, p.level;

-- 2. Merge Joinを誘発するインデックスを作成
-- guild_id でソートされ、内側では level でソートされている複合インデックス
CREATE INDEX idx_players_guild_level ON players(guild_id, level);

-- 3. guild テーブルにも同様のインデックスがあれば効果的
-- （guild はサイズが小さいため、この例では効果が限定的）

-- 4. 再度実行計画を確認（Merge Join が選ばれるはず）
EXPLAIN ANALYZE 
SELECT p.id, p.name, p.level, g.guild_name 
FROM players p 
JOIN guilds g ON p.guild_id = g.id 
WHERE p.level >= 50 
ORDER BY p.guild_id, p.level;</pre>
                </div>
                <div class="code-explanation">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>2～7行目（現状確認）：</strong> WHERE句で「level >= 50」、ORDER BYで「guild_id, level」でソート結果を指定しています。統計情報から判断して、オプティマイザは「全体の約50%が該当する」と予想し、Hash Joinを選ぶ可能性が高いです</li>
                        <li><strong>10～11行目（インデックス作成）：</strong> (guild_id, level)の複合インデックスを作成します。このインデックスは「guild_idで整理され、その中でlevelでソート」されているため、ORDER BY句とマッチして、Merge Joinを可能にします</li>
                        <li><strong>14行目（コメント説明）：</strong> guildsテーブルはサイズが小さい（50行）ため、結合方式の選択への影響は限定的です。重要なのはplayersテーブル側のインデックス設計です</li>
                        <li><strong>17～22行目（改善後の確認）：</strong> インデックス作成後、同じクエリを実行すると、オプティマイザは「あ、すでにソート済みのインデックスがある」と認識し、新たにソートせずにMerge Joinを選びます</li>
                    </ul>
                </div>
                <p>
                    <strong>解説：</strong> 
                    ORDER BY で guild_id と level でソートされているため、
                    複合インデックス(guild_id, level)を作成すると、
                    オプティマイザが Merge Join を選びやすくなります。
                </p>
            </details>

            <h3>【演習4】統計情報とオプティマイザ（難易度：⭐⭐⭐）</h3>
            <div class="drill-box">
                <p>
                    以下の手順を実行し、統計情報が実行計画にどう影響するか観察してください。
                </p>
                <ol>
                    <li>大量のプレイヤーレコードを INSERT する</li>
                    <li>ANALYZE を実行「前」「後」で、同じクエリの rows 予測値を比較</li>
                    <li>オプティマイザの判断がどう変わるか説明する</li>
                </ol>
            </div>

            <details>
                <summary>演習4の解答例を表示</summary>
                <div class="code-block">
                    <pre>-- 1. 大量の新規データを挿入（レベルが高いプレイヤーばかり）
INSERT INTO players (id, name, level, guild_id, login_count, created_at)
SELECT 
    200000 + i,
    'HighLevel_' || i,
    floor(random() * 30 + 71),  -- Lv 71-100 に集中
    floor(random() * 50 + 1),
    floor(random() * 500),
    CURRENT_TIMESTAMP
FROM generate_series(1, 50000) i;

-- 2. ANALYZE なしで実行計画を確認（予測値が古い）
EXPLAIN SELECT * FROM players WHERE level >= 80;

-- 3. ANALYZE を実行
ANALYZE players;

-- 4. 実行計画を再確認（rows 予測値が更新される）
EXPLAIN SELECT * FROM players WHERE level >= 80;</pre>
                </div>
                <div class="code-explanation">
                    <h5>📍 コード解説</h5>
                    <ul>
                        <li><strong>2～10行目（大量挿入）：</strong> 「200000 + i」により、新規プレイヤーID 200001～250000を追加します。注意すべき点は「Lv 71-100に集中」していることです。元のデータは「Lv 1-100が均等」だったため、分布が大きく変わります</li>
                        <li><strong>13行目（ANALYZE前の確認）：</strong> 統計情報はまだ「旧分布」のままなので、「WHERE level >= 80」の推定行数が過小評価されます。元の均等分布なら「約10,000の20%=2,000行」と予測されますが、実際には新しいデータが大量に追加されています</li>
                        <li><strong>16行目（ANALYZE実行）：</strong> 統計情報を最新化します。PostgreSQLがテーブルをサンプリングして「新しい分布はレベルが高い方に偏っている」という情報を認識します</li>
                        <li><strong>19行目（ANALYZE後の確認）：</strong> 「WHERE level >= 80」のrows予測値が大幅に増加します。新たな実行計画では、より正確な見積もりに基づいて結合方式が選ばれるようになります</li>
                    </ul>
                </div>
                <p>
                    <strong>期待される結果：</strong> 
                    ANALYZE 後、rows の予測値が増加します。
                    これにより、オプティマイザはより正確な計画を立てられるようになります。
                </p>
            </details>
                </p>
            </details>

        <hr>
        <section id="conclusion">
            <h2>まとめ：実行計画から見えるデータベース設計の本質</h2>
            <p>
                本記事を通じて、SQLの実行計画をコントロールすることが、
                いかにシステムのパフォーマンスに直結するかを学びました。
            </p>
            <p>
                データベースエンジニアの最重要スキルは「推測と検証」です：
            </p>
            <ol>
                <li><strong>推測：</strong> 「このクエリは遅そうだ。なぜなら...」と仮説を立てる</li>
                <li><strong>検証：</strong> EXPLAIN ANALYZE で実測値を確認する</li>
                <li><strong>改善：</strong> インデックス設計やクエリ書き直しで最適化</li>
                <li><strong>再検証：</strong> 改善後の実行計画を確認し、効果を計測</li>
            </ol>
            <p>
                本科目で学んだインデックス、統計情報、結合アルゴリズムといった知識が、
                この「推測と検証」のサイクルを支えています。
            </p>
        </section>

        <footer>
            <p>作成：DB工学 技術解説プロジェクト</p>
            <p><strong>PostgreSQL 17</strong> | <strong>実行計画</strong></p>
            <p>想定学習時間：約90分（読解 + 実験 + 演習）</p>
            <p style="font-size: 0.85rem; color: #64748b; margin-top: 1.5rem;">📝 <em>作成所要時間：約18時間</em></p>
        </footer>
    </main>
</body>
</html>